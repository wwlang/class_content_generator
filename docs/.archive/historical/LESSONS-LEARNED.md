# Lessons Learned - Test Run & Phase 1

**Date:** November 24, 2025
**Context:** First complete system test with BCI2AU Business Communication course

---

## Executive Summary

During the initial test run, we discovered **5 critical workflow issues** that would have led to poor course quality and wasted time. All issues were fixed in Phase 1, with architectural improvements that prevent them from recurring.

**Key insight:** Test runs are invaluable. What seemed like straightforward workflows had subtle but critical flaws that only became apparent during real-world usage.

---

## The 5 Critical Issues

### Issue 1: Textbook Dependency ðŸ“š

**What Happened:**
- I defaulted to using Guffey's "Business Communication" textbook structure
- Synthesized course outline from textbook table of contents
- Ignored the approved course description promises

**User Feedback:**
> "I absolutely hate that textbook. Don't base anything on that."

**Why It Happened:**
- Textbooks are common references in course design
- I assumed textbook = authoritative source
- Didn't validate against user's actual requirements

**Root Cause:** No validation checkpoint between description promises and course structure

**The Fix:**
1. **Step 1.5 added:** Extract Description Promises
   - Parse approved description for specific topics
   - Create explicit checklist (e.g., "AI and emerging technologies")
   - Validate structure maps to every promise
2. **Warnings added:** "DO NOT use textbook table of contents as primary structure"
3. **Priority clarified:** Top schools > User requirements > Textbooks

**Prevention:**
- Validation checkpoint catches misalignment early
- Clear hierarchy: User promises â†’ Top schools â†’ Research
- Textbooks only for supplementary reference

**Lesson:** Always validate generated structure against approved requirements, not assumptions.

---

### Issue 2: Missing Description Promises ðŸŽ¯

**What Happened:**
- Generated 10-week structure covering traditional business communication
- Missing: AI content, crisis management, virtual collaboration
- All three were explicitly mentioned in approved description

**User Feedback:**
> "you seem to be missing the AI content we discussed in the course description"
> "Did you ensure this had enough correlation to the course description I approved - if not, how can we ensure this in future?"

**Why It Happened:**
- Generated structure from synthesis of top schools + textbook
- Never explicitly checked: "Did I include everything I promised?"
- Assumed comprehensive coverage = quality coverage

**Root Cause:** No explicit promise extraction and validation mechanism

**The Fix:**
1. **NEW Step 1.5:** Extract Description Promises
   ```markdown
   â–¡ AI and emerging technologies (Week X)
   â–¡ Crisis communication (Week Y)
   â–¡ Virtual collaboration tools (Week Z)
   ```
2. **Step 2 validation:** Course structure must address EVERY checkbox
3. **Documentation:** Save checklist to `.working/description-promises-checklist.md`

**Prevention:**
- Machine-readable checklist (not just human review)
- Explicit validation in Step 2 before proceeding
- Audit trail for why each topic included

**Lesson:** Explicit promises require explicit validation. Never assume implicit coverage.

---

### Issue 3: Wrong Pedagogical Order ðŸŽ“

**What Happened:**
- Original workflow: Step 3 = Design Assessments, Step 4 = Research Articles
- Tried to design assessments before knowing what content we'd teach
- Pedagogically backwards

**User Feedback:**
> "weren't we going to do the assessment design after the topic research, as the assessments should follow the content for the course?"

**Why It Happened:**
- Logical fallacy: "We know the topics, so we can design assessments"
- Reality: Topics â‰  Content. Need to know specific frameworks, theories, concepts
- Assumption that assessment design is independent of specific content

**Root Cause:** Fundamental misunderstanding of assessment design principles

**The Fix:**
1. **Reordered steps:** Step 3 = Research, Step 4 = Assessments
2. **Added rationale (generate-syllabus.md lines 188-191):**
   ```markdown
   Assessments must test what we actually teach, not just topic headings.
   Only after researching can we know specific frameworks, theories, and
   skills to assess. This ensures pedagogical alignment.
   ```
3. **Assessment step now references:** "Based on researched content..."

**Prevention:**
- Pedagogical principle enforced in workflow order
- Rationale documented so future updates don't revert
- Assessment design explicitly references researched frameworks

**Lesson:** Assess what you teach, not what you plan to teach. Content must precede assessment design.

---

### Issue 4: Date Assumption Error ðŸ“…

**What Happened:**
- User requested research from "past 6 months" (May-November 2025)
- I calculated: January 2025 - 6 months = July 2024 (wrong!)
- Actual date: November 24, 2025
- Correct calculation: November 2025 - 6 months = May 2025

**User Feedback:**
> "you're assuming incorrectly that it's January 2025 for some reason - please tell me the reason so I understand this mistake in future"

**Why It Happened:**
- Saw "2025" in knowledge cutoff context
- Made assumption: "2025 probably means early in the year"
- Never checked the `<env>` tag that contains actual date

**Root Cause:** Cognitive shortcut + failure to verify available information

**My Explanation to User:**
> "I saw '2025' mentioned in my context and defaulted to assuming we're early in the year (January), rather than checking the actual date in the `<env>` tag. It's a cognitive shortcut where I filled in missing information with a plausible default instead of looking up the precise value."

**The Fix:**
1. **Step 3D added explicit date checking:**
   ```markdown
   1. Check `<env>` tag for today's date
   2. Calculate date ranges explicitly (e.g., "Past 6 months" = [date] to [date])
   3. Use calculated range in search queries
   ```
2. **Never assume date context** - always verify
3. **Document date reasoning** in research notes

**Prevention:**
- Mandatory date verification step
- Explicit date range calculations shown to user
- No assumptions about "current" time

**Lesson:** Verify available information before making assumptions. Date context is critical for currency.

---

### Issue 5: Desktop as Afterthought ðŸ–¥ï¸

**What Happened:**
- Desktop handoff presented as "Option 2" (secondary choice)
- Direct research in Code presented first
- User had to ask: "Do the hand-off prompts include file paths?"

**User Feedback (Implied):**
- Questions about automation suggested workflow wasn't smooth enough
- Desktop should be PRIMARY method, not alternative
- File path questions revealed missing automation

**Why It Happened:**
- Initial workflow designed around Code (my primary environment)
- Desktop added later as enhancement
- Didn't fully embrace Desktop's superior capabilities

**Root Cause:** Tool-centric design (Code-first) instead of outcome-centric (best quality)

**The Fix:**
1. **Reordered options:**
   - **Option 1 (RECOMMENDED):** Claude Desktop Handoff
   - **Option 2 (Fallback):** Direct research in Code
2. **Added time comparisons:**
   - Desktop: 7-10.5 hrs total (2-2.5 Code + 5-8 Desktop)
   - Direct: 6-9.5 hrs (all in Code)
3. **Built Desktop-first automation:**
   - Master prompt generator (one prompt for 10 weeks)
   - MCP auto-write (no copy/paste)
   - Flag files for validation
4. **Documentation updated:** Desktop benefits highlighted throughout

**Prevention:**
- Always present best method first (quality > convenience)
- Build automation around recommended workflow
- Time comparisons help users make informed choices

**Lesson:** Design for the best outcome, not the most familiar tool. Recommend what's best, not what's easiest.

---

## Cross-Cutting Themes

### Theme 1: Validation Checkpoints Are Critical

**Issues:** 1, 2, 5
**Pattern:** Generated content without validating against requirements

**Solution:**
- Step 1.5: Validate description promises
- Step 2: Validate structure maps to promises
- Step 3B: Batch validate research before proceeding
- Phase 2: Validate research before generating content

**Principle:** Validate early, validate often. Catch errors before they compound.

---

### Theme 2: Explicit Over Implicit

**Issues:** 1, 2, 4
**Pattern:** Made assumptions instead of explicit verification

**Solution:**
- Checklists for promises (not just "review")
- Date verification in `<env>` (not assumptions)
- Mapping structure â†’ promises (explicit documentation)

**Principle:** If it's important, make it explicit. Don't rely on implicit understanding.

---

### Theme 3: Pedagogy Over Process

**Issues:** 3, 5
**Pattern:** Process efficiency over pedagogical correctness

**Solution:**
- Research â†’ Assessments ordering (pedagogically sound)
- Desktop-first (quality over convenience)
- Rationale documented for future reference

**Principle:** Educational quality trumps operational efficiency. Get the pedagogy right first.

---

### Theme 4: User Requirements Are Sacred

**Issues:** 1, 2, 4
**Pattern:** System defaults overriding user specifications

**Solution:**
- Description promises as validation checklist
- User-specified date ranges calculated explicitly
- Top schools + user requirements > any default

**Principle:** The user's explicit requirements always override system defaults or assumptions.

---

## Architectural Improvements

### Improvement 1: Validation-Driven Workflow

**Before:** Generate â†’ Review â†’ Fix if wrong
**After:** Validate â†’ Generate â†’ Auto-validate â†’ User review

**Benefits:**
- Catch errors before wasting time generating
- Automated validation catches more than human review
- Compound errors prevented (bad research â†’ bad content)

---

### Improvement 2: Desktop-First Design

**Before:** Code-centric with Desktop as alternative
**After:** Desktop-first with Code as fallback

**Benefits:**
- Better research quality (Desktop's superior capabilities)
- Parallelization (10 agents simultaneously)
- Time efficiency (master prompt vs. sequential)

---

### Improvement 3: Explicit Promise Tracking

**Before:** Description â†’ Structure (implicit mapping)
**After:** Description â†’ Promises â†’ Structure (explicit validation)

**Benefits:**
- Machine-readable validation (not just human review)
- Audit trail (why each topic included)
- Prevents missing promised content

---

### Improvement 4: Pedagogical Ordering

**Before:** Topics â†’ Assessments â†’ Research
**After:** Topics â†’ Research â†’ Assessments (based on content)

**Benefits:**
- Assessments test actual content taught
- Alignment between readings and graded work
- Pedagogically sound progression

---

## Testing Insights

### Insight 1: Real-World Testing Reveals Hidden Issues

**Observation:** All 5 issues only became apparent during actual course generation
**Conclusion:** Theoretical workflow review â‰  practical testing
**Action:** Always test with real courses before declaring "production-ready"

---

### Insight 2: User Feedback is Gold

**Observation:** User caught every issue immediately during review
**Conclusion:** Expert users notice quality gaps AI might miss
**Action:** Build checkpoints for user review at critical junctions

---

### Insight 3: Documentation Promises Create Accountability

**Observation:** User referenced "(if exists)" notation for template
**Conclusion:** Documentation promises are commitments
**Action:** Audit documentation claims and fulfill all promises

---

### Insight 4: Explicit Rationale Prevents Regression

**Observation:** Pedagogical ordering might be "optimized" back to wrong order
**Conclusion:** Document WHY decisions were made
**Action:** Add rationale comments in workflows for non-obvious choices

---

## Prevention Checklist for Future Workflows

Use this checklist when designing new workflows:

### Design Phase
- [ ] Identify user requirements explicitly (not implicit)
- [ ] Map requirements to validation checkpoints
- [ ] Design for best outcome (not most familiar tool)
- [ ] Verify pedagogical soundness (if educational)
- [ ] Document rationale for non-obvious choices

### Implementation Phase
- [ ] Add validation checkpoints before generation
- [ ] Verify information instead of assuming (dates, defaults)
- [ ] Create machine-readable checklists (not just "review")
- [ ] Build automation for recommended workflow (not just fallback)
- [ ] Document "why" not just "how"

### Testing Phase
- [ ] Test with real-world scenarios (not toy examples)
- [ ] Get expert user feedback on generated content
- [ ] Verify all documentation promises fulfilled
- [ ] Check for compound errors (bad input â†’ bad output)
- [ ] Test fallback workflows (not just happy path)

### Documentation Phase
- [ ] Document validation requirements explicitly
- [ ] Include rationale for non-obvious design choices
- [ ] Audit all template/file references for existence
- [ ] Provide troubleshooting for failure modes
- [ ] Update cross-references in related docs

---

## Quotes to Remember

> "I absolutely hate that textbook. Don't base anything on that."
**Lesson:** User requirements > standard references

> "you seem to be missing the AI content we discussed in the course description"
**Lesson:** Explicit promises require explicit validation

> "weren't we going to do the assessment design after the topic research"
**Lesson:** Pedagogy trumps process convenience

> "the objective of this test run is not primarily to create course content, but to refine the generators"
**Lesson:** Testing is about improving the system, not just output

---

## Success Metrics

**Before Phase 1:**
- 5 critical workflow issues
- No validation checkpoints
- Process-centric design
- Implicit assumptions

**After Phase 1:**
- All 5 issues fixed with prevention mechanisms
- 4 validation checkpoints added
- Outcome-centric design (Desktop-first)
- Explicit verification required

**Result:** System transformed from "works in theory" to "production-ready with safeguards"

---

## What We'd Do Differently Next Time

1. **Test earlier:** Don't wait until "complete" to test with real courses
2. **User validation checkpoints:** Build in user review at critical junctions
3. **Template audit first:** Verify all referenced files exist before documentation
4. **Pedagogy review:** Always check educational soundness before efficiency
5. **Explicit over implicit:** When in doubt, make it explicit and verifiable

---

## Key Takeaway

**The system works when it works for the user, not when it works in theory.**

Test early, listen to feedback, validate explicitly, and prioritize pedagogical correctness over operational efficiency. These principles transformed a flawed workflow into a production-ready system.

---

*Lessons learned November 24, 2025 - First complete test run*
