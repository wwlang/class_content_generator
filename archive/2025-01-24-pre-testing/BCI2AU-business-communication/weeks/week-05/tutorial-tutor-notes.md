# Week 5 Tutorial: Tutor Notes

**Course:** Business Communication (BCI2AU)
**Tutorial:** AI Ethics in Business Communication
**Duration:** 90 minutes

---

## Tutorial Goals

- Prepare students to use AI ethically and strategically in their Written Portfolio submissions (due this week)
- Develop ethical reasoning skills for AI use decisions in professional contexts
- Reinforce course framework application (Cialdini, BLUF, crisis principles) as distinct from generic AI output

## Key Outcomes

By the end of this tutorial, students should be able to:
- Distinguish between appropriate and inappropriate AI use contexts for business communication
- Apply the four ethical principles (transparency, accountability, authenticity, skill development) to evaluate AI use scenarios
- Recognize when AI assistance undermines vs. enhances professional communication quality
- Articulate the strategic value they add beyond AI-generated content
- Make informed decisions about AI disclosure and verification practices

---

## Quiz Solutions

### Question 1: AI Research Findings
**Correct: B) 14%**

**Why B is correct:**
Brynjolfsson et al. (2023) found an average 14% increase in issues resolved per hour across all 5,179 customer support agents in the randomized controlled trial. This is the overall productivity gain mentioned in Slide 9 of the lecture.

**Why wrong:**
- A (5%): Too low—understates the significant productivity impact found in the study
- C (35%): This was the gain for *newer workers specifically*, not the average across all workers. Common confusion between overall average and subgroup results
- D (50%): Not supported by research—exaggerates the findings

**Common confusion:** Students confuse the 14% average with the 35% gain for novice workers (also mentioned in the same study). Emphasize that 14% is the *overall average* across all experience levels, while 35% represents the *maximum gain* for the least experienced group.

**Teaching tip:** Remind students that newer workers benefit most from AI because they learn expert patterns from AI suggestions (like having a mentor). Experienced workers already know these patterns, so AI adds less value.

---

### Question 2: Learning from AI
**Correct: C) Newer workers with less experience**

**Why C is correct:**
The Brynjolfsson et al. (2023) study found that newer workers gained 35% productivity boost (the highest gain of any group), while experienced workers saw minimal productivity change. This demonstrates that AI helps close the skill gap by showing novices expert-level response patterns. (Lecture Slide 9)

**Why wrong:**
- A (Experienced workers 10+ years): Actually gained almost nothing (~0%)—they already knew best practices
- B (Mid-level workers 3-5 years): Study didn't report mid-level separately; they fall between novice and expert
- D (All equally): False—the key finding was differential impact based on experience level

**Common confusion:** Students might assume experienced workers benefit most because they know how to use tools better. Reality is opposite: novices benefit most because AI teaches them expert patterns they don't yet know.

**Teaching tip:** Use analogy: "AI is like having an expert looking over your shoulder. If you're already an expert, you don't learn much new. If you're a beginner, every suggestion teaches you something." This mirrors the 70-20-10 learning model where 20% comes from coaching/mentoring.

---

### Question 3: Ethical Principles
**Correct: C) Efficiency (complete tasks as quickly as possible)**

**Why C is correct:**
Efficiency is NOT one of the four ethical principles. The four principles are: (1) Transparency, (2) Accountability, (3) Authenticity, and (4) Skill Development (Lecture Slides 12-15). While efficiency is a *benefit* of AI use, it's not an *ethical principle* for responsible use.

**Why wrong:**
- A (Transparency): Correct principle—disclose AI use appropriately
- B (Accountability): Correct principle—you own results, must verify
- D (Authenticity): Correct principle—preserve unique voice and expertise

**Common confusion:** Students think "efficiency" is a principle because lecture discussed productivity gains. Clarify distinction: Efficiency is an *outcome* of using AI, but ethics focus on *how* you use it (with transparency, accountability, authenticity, and learning mindset).

**Teaching tip:** Draw two columns on board: "Benefits of AI" (efficiency, speed, consistency) vs. "Ethical Principles for AI" (the four principles). This visual separation helps students distinguish outcomes from responsibilities.

---

### Question 4: Human Accountability
**Correct: C) No, AI tools cannot be listed as authors**

**Why C is correct:**
Committee on Publication Ethics (COPE) 2024 guidelines explicitly state that AI tools cannot be listed as authors because they have no expertise, cannot be held accountable, and cannot take responsibility for content. Only humans can be authors. (Lecture Slide 13, citing COPE)

**Why wrong:**
- A (Yes, if >50%): False—percentage of contribution doesn't matter; AI still can't be an author
- B (Yes, with disclosure): False—disclosure is required when AI assists, but AI still isn't listed as author
- D (Only certain documents): False—applies to all professional/academic work

**Common confusion:** Students think "disclosure" means listing AI as co-author. Clarify: You disclose *how* AI was used (in methods/acknowledgments), but AI is never listed in author byline. Human authors take accountability.

**Teaching tip:** Explain analogy: "If you use spell-check, you don't list Microsoft Word as co-author. Similarly, even if AI writes a draft, YOU are the author because you're accountable for the final work. AI is a tool you used, not a collaborator."

---

### Question 5: When NOT to Use AI
**Correct: C) Writing crisis communication with confidential client data**

**Why C is correct:**
This violates multiple principles from the decision framework (Lecture Slide 20): (1) Confidential data should never be put into AI tools due to privacy risks, (2) Crisis communication requires high accountability and deep context only the human communicator has, and (3) High stakes demand human judgment. This is the LEAST appropriate use of AI among the options.

**Why wrong:**
- A (Grammar checking): Most appropriate AI use—always acceptable, no disclosure needed
- B (Brainstorming): Appropriate use—low risk, AI helps generate ideas you then evaluate
- D (Outline structure): Appropriate use—AI helps organization, you control content

**Common confusion:** Students might think time pressure (crisis context) justifies AI use for speed. Emphasize that combining confidential data + crisis stakes + lack of verification makes this extremely inappropriate. Crisis needs accuracy and deep context—can't be rushed with AI shortcuts.

**Teaching tip:** Reference Scenario 2 from tutorial (crisis under time pressure) where student used AI but didn't verify technical details, leading to incorrect information sent to client. Speed doesn't justify sacrificing accuracy and confidentiality.

---

### Question 6: Vietnamese AI Landscape
**Correct: C) 27%**

**Why C is correct:**
Lecture Slide 17 cited 2024 data showing Vietnam hosts 27% of Southeast Asia's Generative AI startups, trailing only Singapore. This demonstrates Vietnam's leadership position in ASEAN AI innovation.

**Why wrong:**
- A (10%): Too low—understates Vietnam's significant AI startup ecosystem
- B (18%): Not the correct figure from the research cited
- D (45%): Too high—this would exceed Singapore's share

**Common confusion:** Students may not remember specific statistic. This tests whether they paid attention to Vietnamese context in lecture.

**Teaching tip:** Connect to student relevance: "Vietnam is #2 in ASEAN for AI startups, which means YOU entering job market in next few years will work in AI-fluent companies. Learning ethical, strategic AI use now gives you competitive advantage."

---

### Question 7: Cultural Adaptation
**Correct: C) Relationship language and hierarchy respect (quan hệ, face-saving)**

**Why C is correct:**
Lecture Slide 18 emphasized that AI (trained on Western business communication) produces low-context, direct messages that miss Vietnamese cultural elements like relationship references, hierarchical respect (anh/chị, kính gửi), and face-saving language. This requires the MOST human adaptation because AI fundamentally doesn't understand high-context, relationship-oriented culture.

**Why wrong:**
- A (Grammar and spelling): AI handles this well for English (this is AI's strength)
- B (Document formatting): AI can follow standard formats adequately
- D (Technical terminology): AI has good vocabulary for technical terms, though PhoGPT better for Vietnamese

**Common confusion:** Students might think all cultural factors need equal adaptation. Emphasize that surface elements (grammar, format) are easy for AI, but deep cultural elements (relationships, hierarchy, face-saving) require human cultural intelligence.

**Teaching tip:** Reference Scenario 3 from tutorial where student used AI's direct Western-style email to Vietnamese partner, causing offense. Ask: "What did AI miss?" (Answer: Relationship language, face-saving for schedule conflict, hierarchical respect). This illustrates why cultural adaptation is human's responsibility.

---

### Question 8: The Productivity Paradox
**Correct: B) Tools that save time now may reduce skill development later**

**Why B is correct:**
Lecture Slide 15 explained the Productivity Paradox: AI tools increase efficiency in the short term, but over-reliance may prevent skill development in the long term. If you always let AI write persuasive messages, you never learn persuasion yourself. This creates dependence on tools rather than building expertise.

**Why wrong:**
- A (Less productive than claimed): Not the paradox—research shows real productivity gains
- C (Gains disappear when everyone uses): Not the paradox discussed, though interesting economic question
- D (Some faster, some slower): Not the paradox—doesn't capture the time-now vs. skills-later tension

**Common confusion:** "Paradox" sounds negative, so students might pick negative-sounding answers. Clarify: Paradox means contradictory outcomes (good short-term, potentially bad long-term if misused). It's not about AI being bad—it's about needing strategic use to avoid dependence.

**Teaching tip:** Use analogy: "Like using GPS navigation for driving. Short-term: You reach destination efficiently. Long-term: If you ALWAYS use GPS, you never learn the routes yourself, and you're lost if GPS fails. Paradox = Efficiency now, dependence later. Solution: Use GPS but also learn the routes."

---

### Question 9: Appropriate AI Use
**Correct: B) Use ChatGPT to brainstorm persuasive angles, then write proposal yourself applying Cialdini's principles**

**Why B is correct:**
This follows the recommended workflow (Lecture Slide 21): YOU handle strategy (applying Cialdini's principles, audience analysis), AI assists with ideation (brainstorming angles). This maintains course framework application, authenticity, and accountability while benefiting from AI's brainstorming capacity. Aligns with ethical principles.

**Why wrong:**
- A (AI writes, minor edits): Violates authenticity and accountability; doesn't demonstrate YOUR mastery of course frameworks
- C (Copy competitor proposal): Multiple violations—confidentiality, originality, potentially plagiarism
- D (AI writes, no disclosure): Violates transparency principle; also doesn't show course framework application

**Common confusion:** Students might think option A is okay if they "edit" AI output. Emphasize: The rubric grades YOU on strategic framework application. AI can't apply course concepts—only you can. If AI wrote it, you didn't demonstrate mastery.

**Teaching tip:** Reference Written Communication Rubric criterion "Masterfully applies strategic frameworks from course." Ask: "If ChatGPT writes proposal, who applied Cialdini's principles—you or ChatGPT?" Answer: ChatGPT might mention persuasion, but YOU must strategically select which principles to use and why for specific audience. That's what rubric evaluates.

---

### Question 10: Disclosure Requirements
**Correct: C) Whenever AI is used beyond basic grammar/spell checking**

**Why C is correct:**
Lecture Slide 12 (Transparency principle) stated: AI use beyond standard grammar checking should be disclosed for academic assignments. Grammarly/spell-check are baseline professional tools (like Word spell-check), so don't require disclosure. But generative AI use (brainstorming, drafting, outlining) should be disclosed to show transparency and build good professional habits.

**Why wrong:**
- A (Only if >50%): Disclosure not based on percentage—based on role and type of use
- B (Only generative, not grammar): Partially correct (grammar doesn't need disclosure) but incomplete (generative AI DOES need disclosure)
- D (Never for students): False—academic integrity requires disclosure for student work

**Common confusion:** Students fear disclosure will lower their grade. Reassure: Disclosure shows integrity and professionalism. What lowers grades is (1) not applying course frameworks strategically, or (2) submitting work without disclosure when AI was used (academic dishonesty).

**Teaching tip:** Provide template: "I used [Tool] for [Task: brainstorming/outlining/polishing]. I then [What YOU did: applied course frameworks/wrote content/verified accuracy/added context]." This makes disclosure easy and shows strategic AI use.

---

## Facilitating the Main Activity

**Context:**

The four scenarios are deliberately designed to represent common AI use situations with ethical complexity. There are no universally "right" answers—students must apply ethical reasoning and weigh tradeoffs. The goal is developing judgment, not memorizing rules. Scenarios escalate in complexity: Scenario 1 has clearest violation (confidential data), while Scenario 4 is most ironic (AI writing about AI ethics).

**What Good Work Looks Like:**

- Students identify multiple ethical issues in each scenario (not just one principle)
- Risk assessments are specific and realistic (not generic "something bad happens")
- "Better Approach" workflows show concrete steps: what to do first, what to verify, what to add, how to disclose
- Reasoning is explicit: "This violates accountability because..." (not just "this is bad")
- Students acknowledge tradeoffs: "This approach is slower but more ethical because..."

**Critical Moments:**

**During Individual/Pair Work (25 minutes):**

Around the 10-minute mark, check if pairs are moving through all four scenarios or getting stuck on Scenario 1. If stuck, prompt: "You don't need to write perfect analysis—capture key ethical issues and move to next scenario. You'll revise during debrief." The goal is exposure to all four scenarios, not exhaustive analysis of one.

Watch for pairs that write vague "Better Approach" like "Be more ethical" or "Check with manager." Prompt: "Can you be more specific? What are the actual steps you'd take? What would you verify? How would you use AI differently?" Push for concrete workflows.

For Vietnamese students, Scenario 3 (cross-cultural email) may resonate most personally. Some may have experienced or witnessed similar situations. If you notice engaged discussion about Vietnamese cultural norms (quan hệ, face-saving, hierarchy), let it run 1-2 extra minutes—this is valuable learning. Then redirect: "Great insights—capture those in your analysis and let's make sure we cover all four scenarios."

**During Peer Review (15 minutes):**

Around the 5-minute mark, check if pairs are genuinely engaging with each other's analyses or just skimming. Effective peer review shows evidence: annotations, questions written on worksheets, verbal discussion. If you see silent pairs, prompt: "What's one thing your partner identified that you hadn't thought of? What's one place where you took a different approach and why?"

Watch for pairs that focus only on finding "errors" in each other's work. Redirect: "Remember, multiple valid approaches exist. Your job isn't to find what's 'wrong'—it's to compare reasoning and learn from different perspectives. Where did your partner's logic differ from yours?"

Scenario 4 (LinkedIn post about AI ethics) often generates most debate. Students realize the irony: using AI to write about AI ethics without disclosing violates the very principle being discussed. Let this debate happen—it's the tutorial's most important "aha moment." If it doesn't emerge naturally, prompt during debrief: "What's uniquely problematic about Scenario 4 compared to others?"

**During Revision & Debrief (15 minutes):**

First 10 minutes (revision): Circulate and spot-check revisions. Look for students strengthening analysis with peer insights vs. just rewriting to match peer's answer. Remind: "If you disagree with peer feedback, that's fine—just be able to explain your reasoning clearly."

Final 5 minutes (class debrief): Ask: "Which scenario was most controversial—where did you and your partner disagree most?" Common answers: Scenario 2 (time pressure—does crisis justify shortcuts?) and Scenario 4 (is the irony really that bad?). Facilitate discussion:
- Scenario 2: Acknowledge time pressure is real, but incorrect information to client has consequences. Better approach: Quick call to technical team to verify facts (10 minutes) before sending AI-drafted response. Speed doesn't eliminate accountability.
- Scenario 4: The irony undermines credibility. If you advocate for AI transparency but don't disclose your own AI use, you lose moral authority. Better approach: Either write post yourself (demonstrating expertise) or disclose AI assistance (practicing what you preach).

Connect scenarios to Written Portfolio: "Scenario 1's data privacy issue is why you shouldn't paste real company info into ChatGPT. Scenario 2's verification failure is why you must fact-check AI output. Scenario 3's cultural blindness is why Vietnamese students have competitive advantage—YOU add cultural intelligence AI lacks. Scenario 4's irony is why disclosure matters—it builds trust and credibility."

**Timing Flexibility:**

If running long (common with engaged debate on Scenario 4), you have two options:
1. Shorten quiz prep section to 10 minutes (students answer individually, take answers home, review on own time). Prioritize the scenario analysis—this is higher-value learning.
2. Assign quiz questions as homework before Week 7 (Quiz 2 prep). Post answer key on Moodle for self-study.

If running short (less likely), expand debrief discussion: Ask students to share specific "Better Approach" workflows for Scenarios 2 and 3 (most relevant to portfolio). Have 2-3 pairs share their workflows, compare, discuss tradeoffs.

Priority ranking if time constrained:
1. Main Activity scenario analysis (core learning—can't skip)
2. Peer review (valuable for multiple perspectives—minimize but don't skip)
3. Class debrief (synthesis—critical for connecting to portfolio)
4. Quiz prep (important but can be homework if needed)
5. Wrap-up self-assessment (useful but can be done independently)

---

**END OF TUTOR NOTES**