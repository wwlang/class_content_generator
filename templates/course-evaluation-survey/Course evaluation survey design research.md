# Designing effective university course evaluations: A research-backed framework

The optimal course evaluation survey contains **15-25 items** using **7-point Likert scales**, covers **5-7 validated dimensions**, and employs behavior-focused questions written at a 6th-8th grade reading level. Research demonstrates this configuration maximizes response rates (targeting 70%) while maintaining psychometric validity, with the most effective surveys taking under 10 minutes to complete. The key to actionable results lies in measuring specific, observable teaching behaviors—not global impressions—organized into diagnostic subscales that identify precise areas for improvement.

This synthesis draws from four decades of research on student evaluation instruments, including Herbert Marsh's SEEQ framework (validated across 30+ studies), the IDEA Center's learning-objectives approach, and empirical findings from top business schools. The recommendations below translate this evidence into practical guidance for creating surveys that are accessible to international students while generating genuinely useful feedback for instructors.

## The gold standard instruments share seven core dimensions

The most rigorously validated course evaluation framework is the **Students' Evaluations of Educational Quality (SEEQ)**, developed by Herbert Marsh in 1982. SEEQ has demonstrated remarkable psychometric properties across diverse settings—the USA, Australia, UK, Hong Kong, Spain, India, and beyond—with internal consistency reliabilities (Cronbach's alpha) ranging from **0.88 to 0.97**. Its nine-factor structure has been confirmed through more than 30 factor analyses, and longitudinal studies show stability over 13 years across thousands of courses.

SEEQ measures learning value, instructor enthusiasm, organization and clarity, group interaction, individual rapport, breadth of coverage, examinations and grading, assignments and readings, and workload appropriateness. The instrument contains **35 items total**, including two global ratings for benchmarking. Example items demonstrate the behavior-specific approach that makes SEEQ so diagnostic: "Instructor's explanations were clear," "Students were encouraged to ask questions and were given meaningful answers," and "Feedback on examinations/graded materials was valuable."

The **IDEA Center evaluation system** takes a distinctive approach by anchoring effectiveness to instructor-selected learning objectives rather than teaching behaviors alone. Faculty identify which of 12 learning objectives are essential, important, or minor for their course—from "gaining factual knowledge" to "developing creative capacities" to "learning to analyze and critically evaluate ideas." Students then rate their progress on these objectives alongside 20 specific teaching methods. This customization means a seminar course isn't judged by lecture-hall criteria, and a quantitative methods class isn't expected to develop public speaking skills. IDEA's adjusted scores also control for class size, student motivation, and course difficulty, making cross-course comparisons more meaningful.

Kenneth Feldman's synthesis of SET research identified **organization and clarity** as the dimension most strongly correlated with student achievement (r ≈ 0.56), followed by instructor enthusiasm, student-instructor rapport, and assessment quality. Any well-designed evaluation should prioritize these dimensions. The table below shows how validated instruments align:

| Dimension | SEEQ | IDEA | CEQ | Research Support |
|-----------|------|------|-----|------------------|
| Organization/Clarity | ✓ | ✓ | ✓ | Strongest predictor of achievement |
| Instructor Enthusiasm | ✓ | ✓ | — | Predicts engagement and motivation |
| Student-Instructor Rapport | ✓ | ✓ | — | Critical for learning environment |
| Assessment/Feedback | ✓ | ✓ | ✓ | Essential for learning effectiveness |
| Learning Value/Outcomes | ✓ | ✓ | ✓ | Direct measure of perceived learning |
| Workload Appropriateness | ✓ | — | ✓ | Contextualizes other ratings |
| Course Materials | ✓ | ✓ | — | Important for self-directed learning |

## How top business schools structure their evaluations

Elite MBA programs treat their specific evaluation instruments as proprietary, but research and available examples reveal common patterns. **Chicago Booth** uses a notably concise approach: **9 questions per course**—6 numerical items plus 3 open-ended responses. This brevity contrasts with longer academic instruments but likely contributes to higher completion rates in their quarterly evaluation cycle.

**Harvard Extension School** (whose instrument is publicly documented) employs **5-point Likert scales** with both quality anchors (Very Poor to Excellent) and agreement anchors (Strongly Disagree to Strongly Agree), including a "No Basis for Judgment" option. Their dimensions cover course organization, materials effectiveness, expectation clarity, assessment quality, instructor performance, and TA evaluation—plus contextual questions about enrollment reasons and weekly hours invested.

**Stanford Graduate School of Business** separates evaluations into distinct course feedback and section/TA feedback forms, using their EvaluationKIT platform since 2021. Stanford offers both "Short" and "Long" mid-quarter feedback templates through Canvas, reflecting the growing emphasis on formative assessment that allows instructors to make adjustments while the course is still running.

Business school evaluations include dimensions uncommon in general academic instruments. **Case method effectiveness** appears prominently at schools like HBS and Stanford GSB, measuring discussion quality and analytical skill development. **Career relevance** and **industry application** questions acknowledge that MBA students evaluate courses partly through professional utility. **Team collaboration** items assess group learning dynamics central to business education pedagogy. **Experiential learning quality** addresses project-based and global experience components.

Notably, accreditation bodies like **AACSB and EQUIS do not prescribe specific evaluation instruments**. AACSB focuses on "Assurance of Learning"—systematic processes demonstrating that curriculum achieves intended outcomes—rather than mandating particular questions. This principles-based approach means institutions must design their own frameworks while documenting how assessment results drive improvement.

## Optimal length sits at 15-25 items taking under 10 minutes

Research consistently demonstrates that **survey length inversely predicts response rates**. One study found response rates for short forms averaged **28% higher** than long forms. Surveys under 7 minutes achieve ideal completion rates, while those exceeding 12 minutes see substantial drops. Focus groups with students found they **preferred no more than 15 questions**.

Yet validity requires sufficient items. For scales measuring distinct constructs, **3-5 items per dimension** typically achieve adequate internal consistency (Cronbach's alpha ≥ 0.70). The minimum number of student raters needed for reliability above 0.70 is **7 students per class**—an important consideration for small seminars. Balancing these constraints yields the **15-25 item sweet spot**: enough to cover 5-7 dimensions diagnostically while completing in under 10 minutes.

For response formats, a comprehensive literature review in the International Journal of Educational Methodology (2022) analyzing 60 studies concluded that **7-point scales produce optimal reliability and validity**—significantly outperforming 5-point scales. Churchill and Peter (1984) found 7-point scales improve both metrics compared to 5-point alternatives, while Miller's research demonstrates that reliability does not increase beyond 7 points because individuals cannot reliably distinguish more than seven response levels. However, 5-point scales show advantages for respondent comfort and may reduce frustration, particularly for non-native speakers. Krosnick's research recommends **5 points for unipolar scales** (measuring intensity from none to extreme) and **7 points for bipolar scales** (measuring direction from negative to positive).

**Fully-labeled scales** (verbal anchors for all points, not just endpoints) show higher reliability than endpoint-only formats. Example anchors: "1-Very Poor, 2-Poor, 3-Below Average, 4-Average, 5-Above Average, 6-Good, 7-Excellent."

For open-ended questions, research suggests limiting them to **no more than one-third of total items**—practically, this means **1-2 open-ended questions** for a 15-25 item survey. Place them at the end to avoid influencing quantitative responses, and make them optional to prevent frustration-driven abandonment. Effective prompts target specific, actionable feedback: "What aspects of this course contributed most to your learning?" and "What specific improvements would you suggest?" outperform generic "Any other comments?" prompts.

## Writing items accessible to non-native English speakers

Survey items should target a **6th-8th grade reading level**, per NHMRC and Federal Plain Language Guidelines. This matters enormously for international students, whose comprehension issues with SET surveys affect response validity. The Cross-Cultural Survey Guidelines recommend extensive pretesting with diverse populations and avoiding assumptions about shared cultural reference points.

Keep sentences under **20 words** (absolute maximum 25). Use common, concrete vocabulary—words of **2-3 syllables** when possible. Employ active voice: "The instructor explained concepts clearly" rather than "Concepts were clearly explained by the instructor." Spell out acronyms on first reference. Avoid idioms entirely: "The instructor knocked it out of the park" becomes "The instructor explained concepts very effectively."

**Double-barreled questions**—which ask about two topics in one item—create particular problems for ESL respondents and compromise measurement validity for everyone. The question "Did the instructor explain concepts clearly and provide helpful feedback?" is unanswerable if one applies and the other doesn't. Split into: "The instructor's explanations were clear" and "The instructor provided helpful feedback."

Ambiguous frequency terms require specification. "Sometimes," "frequently," and "often" mean different things to different people; replace with concrete frequencies like "once per week" or "2-3 times per class." Similarly, avoid subjective adjectives like "adequate," "fair," or "good" without behavioral anchors.

**Before and after examples** illustrate these principles:

| Complex (Avoid) | Accessible (Use) |
|-----------------|------------------|
| "The instructor demonstrated a proclivity for utilizing pedagogically sound methodologies in facilitating comprehension of complex subject matter." | "The instructor used effective teaching methods that helped me understand difficult topics." |
| "The instructor was available." | "The instructor responded to emails within 48 hours." |
| "The course materials were helpful." | "The assigned readings helped me understand the lecture topics." |

## Behavior-focused items generate actionable diagnostic data

The fundamental distinction between useful and useless evaluation items is whether they measure **observable behaviors** or **global impressions**. "The instructor was effective" tells faculty nothing about what to change. "The instructor clearly stated learning objectives at the beginning of each class" identifies a specific, modifiable practice.

Research on Behaviorally-Anchored Rating Scales (BARS) demonstrates that specific behavioral anchors improve inter-rater reliability, reduce bias, and provide feedback directly linked to teaching competencies. Trait-focused items like "The instructor was knowledgeable" or "The instructor cared about students" describe internal states that cannot be directly observed or easily modified.

**Transform trait items into behavioral items:**

| Trait-Focused (Avoid) | Behavior-Focused (Use) |
|----------------------|------------------------|
| "The instructor is good." | "The instructor clearly explains how to complete assignments." |
| "The lectures were boring." | "The instructor incorporates activities that allow students to apply concepts." |
| "The instructor knows the material." | "The instructor answers student questions accurately and completely." |
| "The instructor was unhelpful." | "The instructor was available during posted office hours." |
| "The course was hard." | "The workload expectations were clearly communicated at the start of the course." |

**Subscale organization** enables diagnostic use. Rather than computing a single average, report separate scores for course organization, instructional methods, assessment and feedback, student engagement, and learning outcomes. Research shows that clarity and organization items most strongly predict global ratings—if these subscales show problems, that's where intervention should focus.

Effective reporting includes **frequency distributions** (not just means), **comparison benchmarks** (department, school, university norms), and **longitudinal trends** for the same instructor. Identify courses below predetermined thresholds (e.g., mean below 3.5 on a 5-point scale) and flag significant drops from prior years. Require written improvement plans for identified issues, with follow-up tracking.

## A complete framework with validated example items

Based on the research synthesis, here is a recommended structure covering all seven dimensions specified, with example items adapted from validated instruments:

**Course Materials and Resources (3 items)**
- The assigned readings contributed to my understanding of course topics.
- Course materials (slides, handouts, online resources) were well-organized and easy to access.
- The textbook and supplementary materials were relevant to course objectives.

**Instructor Effectiveness and Teaching Quality (4 items)**
- The instructor's explanations were clear and easy to follow.
- The instructor was enthusiastic about teaching this subject.
- The instructor used examples that helped me understand abstract concepts.
- The instructor encouraged questions and provided meaningful answers.

**Assessment Methods and Feedback (3 items)**
- Exams and assignments tested what was emphasized in the course.
- Grading criteria were clearly communicated in advance.
- I received feedback that helped me understand how to improve.

**Perceived Learning Outcomes (3 items)**
- I learned something I consider valuable in this course.
- My understanding of the subject increased as a result of this course.
- I developed skills I can apply outside this course.

**Course Organization and Structure (3 items)**
- The course was well-organized and followed a logical sequence.
- Learning objectives were clearly stated at the beginning of each unit.
- The syllabus accurately described what we would cover.

**Workload Appropriateness (2 items)**
- The workload was appropriate for the credit hours assigned.
- I had enough time to complete assignments thoughtfully.

**Engagement and Interaction (3 items)**
- Students were encouraged to participate in class discussions.
- The instructor was accessible outside of class time.
- The instructor created a respectful learning environment.

**Global Items (2 items)**
- Overall, how would you rate this course?
- Overall, how would you rate this instructor?

**Open-Ended Questions (2 items, optional)**
- What aspects of this course contributed most to your learning?
- What specific changes would improve this course?

This **23-item framework** covers all required dimensions, uses behavior-focused language, maintains 6th-8th grade accessibility, and should take **8-10 minutes** to complete. Administer online with dedicated class time, provide 2-3 weeks advance notification, and send two reminder emails to target **70% response rates**.

## Conclusion: Evidence points to specificity over simplicity

The research consensus challenges a common assumption—that shorter, simpler evaluations are always better. While brevity matters for response rates, **diagnostic utility requires behavioral specificity**. A 15-item survey of global impressions produces less actionable data than a 23-item survey of observable behaviors organized into subscales.

The most important finding across four decades of SET research is that **organization and clarity** most strongly predict both student satisfaction and actual learning outcomes. Any institution designing course evaluations should ensure this dimension receives adequate measurement. The second key insight is that evaluations designed primarily for personnel decisions (hiring, tenure) differ from those designed for teaching improvement—the latter require finer-grained, behavior-level measurement that tells instructors exactly what to change.

Finally, international student populations demand accessible language that many traditional instruments fail to provide. Adapting validated frameworks like SEEQ for 6th-8th grade reading levels, eliminating idioms and double-barreled items, and testing with diverse respondents ensures that evaluation data reflects actual course quality rather than linguistic comprehension barriers.