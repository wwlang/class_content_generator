---
metadata:
  week: 2
  topic: "Career Skills in the AI Era"
  prepares_for: "Personal Future Skills Plan (40%)"
  source: "lecture-content.md"

questions:
  - id: "w2-q01"
    type: "multiple_choice"
    bloom_level: "remembering"
    topic: "Context Engineering"

    question: |
      According to Andrej Karpathy (June 2025), what term should replace 'prompt engineering' to better describe the skill of working effectively with AI?

    options:
      - key: "A"
        text: "Response engineering"
        feedback: "Incorrect. While responses are important, the key innovation is about engineering the context AI receives, not the responses it produces."

      - key: "B"
        text: "Context engineering"
        feedback: "Correct! Karpathy argued that <b>context engineering</b> better captures the 'delicate art and science of filling the context window with just the right information.'"
        correct: true

      - key: "C"
        text: "Query engineering"
        feedback: "Incorrect. Query engineering suggests focusing on how you ask questions, whereas context engineering emphasizes curating what information AI sees."

      - key: "D"
        text: "System engineering"
        feedback: "Incorrect. System engineering is a different discipline focused on complex systems design, not AI collaboration techniques."

    general_feedback: |
      Context engineering is the foundational skill for effective AI collaboration. Rather than focusing on crafting clever prompts, you should focus on curating what information AI "sees" before it responds. This shift in thinking—from prompt-level to context-level—will make you significantly more effective with any AI tool. Apply this principle in your <b>Assessment 1 Personal Future Skills Plan</b> when describing how you'll use AI for skill development.

  - id: "w2-q02"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Context Engineering - Minimum Effective Dose"

    question: |
      You're preparing to use AI to help draft a cover letter for a specific marketing role. Which approach best demonstrates the <b>Minimum Effective Dose</b> principle from context engineering?

    options:
      - key: "A"
        text: "Paste your entire CV, the complete job description, all your academic transcripts, and every project you've worked on to ensure AI has comprehensive information"
        feedback: "Incorrect. This violates the Minimum Effective Dose principle—providing excessive information dilutes AI's attention and reduces output quality."

      - key: "B"
        text: "Identify the 3 key job requirements and your 2 most relevant experiences, then provide AI with only this curated information to establish clear connections"
        feedback: "Correct! The <b>Minimum Effective Dose</b> principle means finding the smallest set of high-signal information that maximizes desired outcomes."
        correct: true

      - key: "C"
        text: "Give AI just the company name and job title without additional context, trusting it to research the rest"
        feedback: "Incorrect. This provides too little context. AI needs strategic context about your experiences and the role's requirements to produce valuable output."

      - key: "D"
        text: "Provide only your contact information and academic major, allowing AI to generate generic content"
        feedback: "Incorrect. This approach will produce workslop—generic content lacking strategic insight or authentic voice."

    general_feedback: |
      This principle requires you to exercise critical thinking: What information is genuinely relevant? What's just noise? The discipline of identifying what matters most (3 key requirements, 2 most relevant experiences) forces you to do strategic thinking that AI cannot do for you. This is the pilot mindset in action—you curate strategically, AI executes tactically. You'll practice this extensively in tutorial when developing your own career materials.

  - id: "w2-q03"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Context Engineering - Goldilocks Zone"

    question: |
      A student gives AI this instruction: 'Write a professional summary for my CV using exactly 4 sentences of 15 words each, starting each sentence with an action verb from this list: [provides list], and mentioning these specific achievements in this exact order: [provides order].' This instruction violates which context engineering principle?

    options:
      - key: "A"
        text: "Minimum Effective Dose—the instruction is too detailed"
        feedback: "Incorrect. The problem isn't the amount of information but the rigidity of the constraints specified."

      - key: "B"
        text: "Attention Budget—the instruction wastes AI's processing capacity"
        feedback: "Incorrect. While the instruction is problematic, it doesn't relate to how attention is distributed across information."

      - key: "C"
        text: "Goldilocks Zone—the instruction is too rigid and will create brittle outputs"
        feedback: "Correct! The <b>Goldilocks Zone</b> principle warns against overly rigid instructions that remove AI's ability to adapt. Instructions should guide intent while allowing flexibility."
        correct: true

      - key: "D"
        text: "Verification Principle—the instruction doesn't include fact-checking steps"
        feedback: "Incorrect. While verification is important, this instruction's main problem is excessive rigidity, not lack of verification steps."

    general_feedback: |
      Finding the "Goldilocks Zone" between too rigid and too vague is an art developed through practice. Overly rigid instructions might work perfectly once but fail completely when conditions change slightly. The goal is to provide strategic direction ("emphasize data storytelling skills") while allowing AI tactical flexibility in how it presents information. You'll experiment with this balance in tutorial—some attempts will be too vague, some too specific, and you'll learn to calibrate through reflection.

  - id: "w2-q04"
    type: "multiple_choice"
    bloom_level: "remembering"
    topic: "Pilot vs. Passenger Mindset"

    question: |
      In the Stanford and BetterUp Labs research (HBR, September 2025), what are the two mindsets identified for working with AI?

    options:
      - key: "A"
        text: "Expert and novice mindsets"
        feedback: "Incorrect. The research didn't focus on expertise levels but rather on approaches to AI collaboration."

      - key: "B"
        text: "Active and passive mindsets"
        feedback: "Incorrect. While these capture some of the distinction, the research used more specific terminology."

      - key: "C"
        text: "Pilot and passenger mindsets"
        feedback: "Correct! The <b>pilot mindset</b> involves actively steering AI with judgment, while the <b>passenger mindset</b> involves passively accepting AI outputs."
        correct: true

      - key: "D"
        text: "Strategic and tactical mindsets"
        feedback: "Incorrect. The research distinguished approaches based on active steering (pilot) versus passive acceptance (passenger), not strategic versus tactical thinking."

    general_feedback: |
      This distinction is central to your career success in AI-augmented environments. Research shows that workers with pilot mindsets are getting promoted and gaining influence, while those with passenger mindsets are being marginalized or replaced. Every time you work with AI, you're choosing which mindset to adopt. The good news: it's a learnable skill. Tutorial will give you extensive practice adopting pilot mindset for career preparation tasks.

  - id: "w2-q05"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Pilot vs. Passenger Mindset Application"

    question: |
      Three students are using AI to prepare for job interviews:<br><br><b>Student A:</b> 'Generate 20 common interview questions and model answers. I'll memorize these.'<br><b>Student B:</b> 'Here's the role [details] and company [research]. Generate 10 role-specific questions. After I answer each, critique my response and suggest improvements.'<br><b>Student C:</b> 'Tell me what answers employers want to hear for behavioral questions.'<br><br>Which student demonstrates the <b>pilot mindset</b>?

    options:
      - key: "A"
        text: "Student A—they're being comprehensive by preparing 20 questions"
        feedback: "Incorrect. Student A is being a passenger—seeking ready-made answers to memorize rather than developing authentic responses."

      - key: "B"
        text: "Student B—they're using AI for iterative practice with feedback while maintaining strategic direction"
        feedback: "Correct! Student B demonstrates the pilot mindset by providing context, using AI as a practice partner, and planning to develop their own authentic answers through iteration."
        correct: true

      - key: "C"
        text: "Student C—they're being efficient by asking directly what employers want"
        feedback: "Incorrect. Student C is being a passenger—looking for ready-made answers rather than developing genuine understanding and authentic responses."

      - key: "D"
        text: "All three demonstrate pilot mindset because they're all using AI for interview preparation"
        feedback: "Incorrect. Simply using AI doesn't indicate pilot mindset—HOW you use AI determines whether you're piloting or being a passenger."

    general_feedback: |
      This scenario illustrates a common trap: using AI to avoid the hard work of thinking rather than using AI to support your thinking. Student A will memorize generic answers that sound rehearsed. Student C will parrot what AI thinks employers want to hear. Student B will develop genuinely thoughtful answers through iterative practice. Which student would you want to hire? The interview itself is a judgment test—employers are evaluating whether you can think strategically about problems. AI can help you practice demonstrating that capability, but it cannot develop the capability for you.

  - id: "w2-q06"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Human in the Loop vs. Human at the Helm"

    question: |
      According to the lecture, which statement best describes the difference between 'human in the loop' and 'human at the helm'?

    options:
      - key: "A"
        text: "'Human in the loop' means reviewing AI outputs for errors; 'human at the helm' means setting strategic direction before AI involvement and evaluating alignment with strategic intent"
        feedback: "Correct! <b>'Human in the loop'</b> is reactive oversight, while <b>'human at the helm'</b> is proactive strategic direction with judgment throughout the process."
        correct: true

      - key: "B"
        text: "'Human in the loop' means working alongside AI; 'human at the helm' means working without AI assistance"
        feedback: "Incorrect. Both approaches involve working with AI—the difference is the level of strategic control and judgment exercised."

      - key: "C"
        text: "'Human in the loop' applies to technical work; 'human at the helm' applies to creative work"
        feedback: "Incorrect. The distinction isn't about work type but about the level of human judgment and strategic direction involved."

      - key: "D"
        text: "They are synonyms describing the same approach to AI collaboration"
        feedback: "Incorrect. These represent fundamentally different approaches—reactive oversight versus proactive strategic direction."

    general_feedback: |
      The shift from "in the loop" to "at the helm" reflects employers' evolving expectations. As AI becomes more capable, simply checking its outputs is no longer enough—you must be able to set strategic direction, evaluate whether AI's execution matches your intent, and make adjustments. This is why the executives at the Fortune Summit emphasized judgment as the critical skill. Your <b>Assessment 1</b> should demonstrate that you understand how to maintain strategic control while using AI as a tactical tool.

  - id: "w2-q07"
    type: "multiple_choice"
    bloom_level: "remembering"
    topic: "Workslop Statistics"

    question: |
      According to the 2025 Stanford and BetterUp Labs research, what percentage of workers reported encountering 'workslop' from colleagues?

    options:
      - key: "A"
        text: "22%"
        feedback: "Incorrect. 22% felt offended by workslop, but the percentage who encountered it is higher."

      - key: "B"
        text: "41%"
        feedback: "Correct! The research found that <b>41% of workers</b> had encountered workslop from colleagues."
        correct: true

      - key: "C"
        text: "53%"
        feedback: "Incorrect. 53% felt annoyed when receiving workslop, but this is the reaction percentage, not the encounter percentage."

      - key: "D"
        text: "95%"
        feedback: "Incorrect. 95% is the percentage of AI projects that fail to show positive ROI, not the percentage who encountered workslop."

    general_feedback: |
      These statistics reveal that workslop is already widespread—nearly half of workers have experienced it—and that reactions are overwhelmingly negative. This matters for your professional reputation: if you develop a pattern of producing workslop, colleagues will notice, and your career advancement will suffer. The good news: workslop is completely avoidable by adopting the pilot mindset and applying the 30-Second Test before submitting any AI-assisted work.

  - id: "w2-q08"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Identifying Workslop"

    question: |
      A colleague sends you a report that uses phrases like 'In today's fast-paced business environment' and 'It's important to note that,' makes broad claims without specific evidence, and lists several points without connecting them to a strategic recommendation. When you finish reading, you're unclear about the main conclusion. This report most likely:

    options:
      - key: "A"
        text: "Demonstrates excellent professional communication standards"
        feedback: "Incorrect. These are classic workslop indicators—generic phrasing, lack of specifics, unclear conclusions."

      - key: "B"
        text: "Represents workslop—AI-generated content that looks polished but lacks substantive insight"
        feedback: "Correct! This exhibits classic <b>workslop</b> characteristics: generic phrasing, lack of specifics, no clear strategic argument, leaving readers confused."
        correct: true

      - key: "C"
        text: "Shows appropriate use of business writing conventions"
        feedback: "Incorrect. While the phrasing might sound professional, the lack of substance and clarity indicates workslop, not quality business writing."

      - key: "D"
        text: "Indicates the author is an experienced executive who writes efficiently"
        feedback: "Incorrect. Experienced executives write with specificity, clear conclusions, and strategic recommendations—the opposite of what's described here."

    general_feedback: |
      You've likely encountered workslop before without having a name for it—that frustrating experience of reading something that looks professional but leaves you confused about what action to take. Now that you can identify workslop, you can avoid creating it. Before submitting any AI-assisted work, apply the 30-Second Test: Would a colleague immediately recognize thoughtful human work, or would they suspect AI-generated filler? This standard should inform all your coursework and professional communications.

  - id: "w2-q09"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "The 30-Second Test"

    question: |
      You've drafted a cover letter with AI assistance and are applying the <b>30-Second Test</b> before submitting it. Which observation would be a RED FLAG indicating potential workslop?

    options:
      - key: "A"
        text: "The letter mentions a specific project where you increased social media engagement by 45% and explains why this experience is relevant to the target role"
        feedback: "Incorrect. This is a GREEN FLAG—specific, quantified achievement with clear relevance demonstrates thoughtful work."

      - key: "B"
        text: "The opening paragraph could be sent to any company in the industry without changing the content"
        feedback: "Correct! This is a <b>RED FLAG</b> for workslop—generic content indicates lack of research and absence of strategic thinking."
        correct: true

      - key: "C"
        text: "You can clearly articulate why you chose to emphasize certain experiences over others based on the job requirements"
        feedback: "Incorrect. This is a GREEN FLAG—demonstrating strategic thinking and judgment indicates pilot mindset, not workslop."

      - key: "D"
        text: "The letter acknowledges a tradeoff in your background and explains how you'll address it"
        feedback: "Incorrect. This is a GREEN FLAG—honest self-assessment with a plan shows authentic human judgment."

    general_feedback: |
      This test is your quality control mechanism. Generic opening paragraphs are the most common workslop indicator in application materials—they signal to recruiters that you haven't done research or invested thought in this specific opportunity. The other options are GREEN FLAGS because they demonstrate: (1) specific, quantified achievements, (2) strategic thinking about what to emphasize, and (3) honest self-assessment with a plan to address gaps. These are signs of pilot mindset and authentic human judgment.

  - id: "w2-q10"
    type: "multiple_choice"
    bloom_level: "remembering"
    topic: "Explore-Build-Connect-Refine Framework"

    question: |
      In the Explore-Build-Connect-Refine framework for AI-assisted career development, which stage focuses on verification and fact-checking?

    options:
      - key: "A"
        text: "Explore stage"
        feedback: "Incorrect. The Explore stage focuses on discovery—mapping possibilities, identifying gaps, and understanding options."

      - key: "B"
        text: "Build stage"
        feedback: "Incorrect. The Build stage focuses on skill development through iterative practice, not verification."

      - key: "C"
        text: "Connect stage"
        feedback: "Incorrect. The Connect stage focuses on communication, networking, and relationship-building preparation."

      - key: "D"
        text: "Refine stage"
        feedback: "Correct! The <b>Refine stage</b> focuses on quality control, including testing, identifying weaknesses, and critically, verifying all factual claims."
        correct: true

    general_feedback: |
      The Refine stage is non-negotiable and cannot be skipped. AI hallucinates—it will occasionally cite programs that don't exist, misstate deadlines, or confuse companies. YOU must verify every factual claim against primary sources (company websites, official announcements, program documentation). This is especially important for scholarship applications, job applications, and any professional communications where accuracy reflects on your credibility. Make verification a habit: AI assists, but you verify.

  - id: "w2-q11"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Explore-Build-Connect-Refine Application"

    question: |
      You're using the Explore-Build-Connect-Refine framework to prepare for a career fair. You've researched target companies (Explore) and drafted a 60-second elevator pitch (Build). Now you ask AI to 'roleplay as a marketing manager from [target company] who might attend the career fair. I'll deliver my elevator pitch, and you ask follow-up questions a recruiter might ask. After I answer, provide feedback.' Which stage are you in, and what is the primary purpose of this activity?

    options:
      - key: "A"
        text: "Build stage—you're still constructing your elevator pitch"
        feedback: "Incorrect. The pitch is already drafted (Build complete). You're now rehearsing communication for real interactions."

      - key: "B"
        text: "Connect stage—you're rehearsing authentic communication and preparing for real human interaction"
        feedback: "Correct! The <b>Connect stage</b> focuses on communication and networking preparation. AI roleplay helps you rehearse so you can communicate authentically in real interactions."
        correct: true

      - key: "C"
        text: "Refine stage—you're verifying facts in your pitch"
        feedback: "Incorrect. While Refine includes verification, this activity is about practicing communication delivery and handling follow-up questions, which is Connect stage work."

      - key: "D"
        text: "Explore stage—you're discovering what recruiters might ask"
        feedback: "Incorrect. You've moved beyond discovery (Explore) and skill-building (Build) to communication practice (Connect)."

    general_feedback: |
      The Connect stage addresses a key principle: AI can help you PRACTICE communication, but it cannot REPLACE authentic human connection. Using AI to roleplay an informational interview or practice your elevator pitch is excellent preparation—you can iterate, receive feedback, and build confidence without stakes. But when the actual career fair happens, you must bring genuine human connection, read social cues, and adapt in the moment. AI builds your capability; human interaction is where you deploy it.

  - id: "w2-q12"
    type: "multiple_choice"
    bloom_level: "understanding"
    topic: "Verification Principle in Refine Stage"

    question: |
      During the Refine stage, you ask AI to critique your application materials 'as if you're a hiring manager looking for reasons to reject it.' AI identifies that you claimed the target company 'recently launched a sustainability initiative,' but this program was actually launched by a competitor. This scenario illustrates which critical principle?

    options:
      - key: "A"
        text: "AI is always unreliable and should never be used for career preparation"
        feedback: "Incorrect. AI is useful for many career preparation tasks when used with the pilot mindset. The lesson is about verification, not avoiding AI entirely."

      - key: "B"
        text: "The 30-Second Test only applies to writing style, not factual accuracy"
        feedback: "Incorrect. The 30-Second Test includes assessing whether claims are specific and substantiated—factual accuracy is part of quality control."

      - key: "C"
        text: "You must verify all factual claims against primary sources because AI can hallucinate or have outdated information"
        feedback: "Correct! This illustrates the critical <b>verification principle</b> in the Refine stage. AI can help critique, but YOU must verify every factual claim."
        correct: true

      - key: "D"
        text: "The Refine stage is optional if you trust AI's research capabilities"
        feedback: "Incorrect. The Refine stage is absolutely non-negotiable—AI hallucinates and makes errors that can destroy professional credibility."

    general_feedback: |
      This scenario demonstrates why the Refine stage is critical: even when AI is helping you improve your materials, it might introduce errors or have outdated information. Imagine submitting that application and confidently discussing the company's "sustainability initiative" in an interview, only to be corrected by the interviewer—your credibility would be destroyed instantly. The lesson: AI is a powerful assistant for drafting and critique, but YOU must verify facts. Check company websites, recent news, official announcements. Accuracy is your responsibility.

---
